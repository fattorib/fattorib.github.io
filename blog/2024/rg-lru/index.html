<!DOCTYPE html>
<!-- _layouts/distill.html --><html>
  <head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">    <!-- Metadata, OpenGraph and Schema.org -->
    

    <!-- Standard metadata -->
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <title>A fast RG-LRU Kernel | Benjamin  Fattori</title>
    <meta name="author" content="Benjamin  Fattori">
    <meta name="description" content="Based on [*folio](https://github.com/bogoli/-folio) design.
">
    <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website">


    <!-- Bootstrap & MDB -->
    <link href="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/css/bootstrap.min.css" rel="stylesheet" integrity="sha256-DF7Zhf293AJxJNTmh5zhoYYIMs2oXitRfBjY+9L//AY=" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous">

    <!-- Fonts & Icons -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.15.4/css/all.min.css" integrity="sha256-mUZM63G8m73Mcidfrv5E+Y61y7a12O5mW4ezU3bxqW4=" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/academicons@1.9.1/css/academicons.min.css" integrity="sha256-i1+4qU2G2860dGGIOJscdC30s9beBXjFfzjWLjBRsBg=" crossorigin="anonymous">
    <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons">

    <!-- Code Syntax Highlighting -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/github.css" media="" id="highlight_theme_light">

    <!-- Styles -->
    
    <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%F0%9F%8C%86&lt;/text&gt;&lt;/svg&gt;">
    
    <link rel="stylesheet" href="/assets/css/main.css">
    <link rel="canonical" href="https://fattorib.github.io/blog/2024/rg-lru/">
    
    <!-- Dark Mode -->
    
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/native.css" media="none" id="highlight_theme_dark">

    <script src="/assets/js/theme.js"></script>
    <script src="/assets/js/dark_mode.js"></script>
    


    <!-- jQuery -->
  <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script>

    <!-- MathJax -->
  <script type="text/javascript">
    window.MathJax = {
      tex: {
        tags: 'ams'
      }
    };
  </script>
  <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script>
  <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>

    <!-- Distill js -->
    <script src="/assets/js/distillpub/template.v2.js"></script>
    <script src="/assets/js/distillpub/transforms.v2.js"></script>
    <script src="/assets/js/distillpub/overrides.js"></script>
    
  </head>

  <body>
<d-front-matter>
    <script async type="text/json">{
      "title": "A fast RG-LRU Kernel",
      "description": "",
      "published": "December 26, 2024",
      "authors": [
        {
          "author": "Benjamin Fattori",
          "authorURL": "fattorib.github.io",
          "affiliations": [
            {
              "name": "",
              "url": ""
            }
          ]
        }
        
      ],
      "katex": {
        "delimiters": [
          {
            "left": "$",
            "right": "$",
            "display": false
          },
          {
            "left": "$$",
            "right": "$$",
            "display": true
          }
        ]
      }
    }</script>
  </d-front-matter>

  

    <!-- Header -->
    <header>

      <!-- Nav Bar -->
      <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top">
        <div class="container">
          <a class="navbar-brand title font-weight-lighter" href="/"><span class="font-weight-bold">Benjamin </span>Fattori</a>
          <!-- Navbar Toggle -->
          <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation">
            <span class="sr-only">Toggle navigation</span>
            <span class="icon-bar top-bar"></span>
            <span class="icon-bar middle-bar"></span>
            <span class="icon-bar bottom-bar"></span>
          </button>

          <div class="collapse navbar-collapse text-right" id="navbarNav">
            <ul class="navbar-nav ml-auto flex-nowrap">

              <!-- About -->
              <li class="nav-item ">
                <a class="nav-link" href="/">about</a>
              </li>
              
              <!-- Blog -->
              <li class="nav-item active">
                <a class="nav-link" href="/blog/">posts<span class="sr-only">(current)</span></a>
              </li>

              <!-- Other pages -->

              <!-- Toogle theme mode -->
              <li class="toggle-container">
                <button id="light-toggle" title="Change theme">
                  <i class="fas fa-moon"></i>
                  <i class="fas fa-sun"></i>
                </button>
              </li>
            </ul>
          </div>
        </div>
      </nav>

      <!-- Scrolling Progress Bar -->
      <progress id="progress" value="0">
        <div class="progress-container">
          <span class="progress-bar"></span>
        </div>
      </progress>
    </header>


    <!-- Content -->
    <div class="post distill">

      <d-title>
        <h1>A fast RG-LRU Kernel</h1>
        <p></p>
      </d-title>

      <d-byline></d-byline>

      <d-article>
        

        <h1 id="a-fast-rg-lru-kernel">A fast RG-LRU Kernel</h1>

<p><strong>TL;DR:</strong> I develop a IO-aware kernel for the RG-LRU in Triton. When training on sequences greater than 4K tokens, the kernel is faster than PyTorch’s Flash Attention 2 implementation.</p>

<h2 id="introduction">Introduction</h2>

<p>Hawk is an RNN proposed by Google Deepmind in “<strong>Griffin: Mixing Gated Linear Recurrences with Local Attention for Efficient Language Models”.</strong><sup id="fnref:1"><a href="#fn:1" class="footnote" rel="footnote" role="doc-noteref">1</a></sup> At its core is the Real-Gated Linear Recurrent Unit (RG-LRU) layer, a <em>linear</em> and <em>diagonal</em> recurrence. A linear and diagonal recurrence means each element in the hidden state only depends on its corresponding element from the previous timestep.</p>

<p>The equations for computing the recurrence are:</p>

\[\begin{align*}
r_t &amp;= \sigma(W_a x_t + b_a) \\
i_t &amp;= \sigma(W_x x_t + b_x) \\
a_t &amp;= a^{C r_t} \\
h_t &amp;= a_t \odot h_{t-1} + \sqrt{1 -  a_{t}^2} \odot (i_t \odot x_t)
\end{align*}\]

<p>Where \(x_t\) is of shape <code class="language-plaintext highlighter-rouge">[B, L, R]</code> , \(h_t\) is of shape <code class="language-plaintext highlighter-rouge">[B, R]</code> and \(C\) is a trainable parameter of size <code class="language-plaintext highlighter-rouge">R</code>. In PyTorch psuedocode, the forward pass is</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="sh">"""</span><span class="s">Example RG-LRU forward pass:
B: batch size
L: sequence length 
D: model dimension
R: Recurrent dimension
</span><span class="sh">"""</span>

<span class="c1"># forward pass
</span><span class="n">gate_x_BLR</span> <span class="o">=</span> <span class="nf">sigmoid</span><span class="p">(</span><span class="n">x_BLR</span> <span class="o">@</span> <span class="n">W_xRR</span><span class="p">)</span>
<span class="n">gate_a_BLR</span> <span class="o">=</span> <span class="nf">sigmoid</span><span class="p">(</span><span class="n">x_BLR</span> <span class="o">@</span> <span class="n">W_aRR</span><span class="p">)</span>

<span class="n">log_a_BLR</span> <span class="o">=</span> <span class="o">-</span><span class="mf">8.0</span> <span class="o">*</span> <span class="n">gate_a_BLR</span> <span class="o">*</span> <span class="nf">softplus</span><span class="p">(</span><span class="n">C_R</span><span class="p">)</span>
<span class="n">a_BLR</span> <span class="o">=</span> <span class="nf">exp</span><span class="p">(</span><span class="n">log_a_BLR</span><span class="p">)</span>
<span class="n">a_square_BLR</span> <span class="o">=</span> <span class="nf">exp</span><span class="p">(</span><span class="mi">2</span> <span class="o">*</span> <span class="n">log_a_BLR</span><span class="p">)</span>
<span class="n">gated_x_BLR</span> <span class="o">=</span> <span class="n">x_BLR</span> <span class="o">*</span> <span class="n">gate_x_BLR</span>

<span class="n">normalized_x_BLR</span> <span class="o">=</span> <span class="n">gated_x_BLR</span> <span class="o">*</span> <span class="nf">sqrt</span><span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">a_square_BLR</span><span class="p">)</span>

<span class="c1"># compute linear recurrence -&gt; fused into single kernel in Pallas or Triton
</span><span class="n">h_t_BR</span> <span class="o">=</span> <span class="nf">zeros</span><span class="p">(</span><span class="n">B</span><span class="p">,</span><span class="n">R</span><span class="p">)</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="n">L</span><span class="p">):</span>
  <span class="c1"># load a_BLR[:,i] and normalized_x_BLR[:,i] to SRAM
</span>  <span class="n">h_t_BR</span> <span class="o">=</span> <span class="n">a_BLR</span><span class="p">[:,</span><span class="n">i</span><span class="p">]</span><span class="o">*</span><span class="n">h_t_BR</span> <span class="o">+</span> <span class="n">normalized_x_BLR</span><span class="p">[:,</span><span class="n">i</span><span class="p">]</span> 
  <span class="c1"># write h_t_BR to HBM
</span></code></pre></div></div>

<p>As the recurrence itself performs no matrix-multiplications and only elementwise operations, it follows that the runtime of this kernel will be memory-bound on any GPU or TPU. The authors develop a custom Pallas kernel to compute this linear recurrence, which can be viewed on DM’s “RecurrentGemma” Github <a href="https://github.com/google-deepmind/recurrentgemma/blob/main/recurrentgemma/jax/pallas.py#L475" rel="external nofollow noopener" target="_blank">here</a>. On the systems side, an interesting takeaway from this paper is that a “naive” fused linear scan kernel actually outperforms the more parallelizeable associative scan kernel. <a href="https://x.com/SonglinYang4/status/1763598848130453728" rel="external nofollow noopener" target="_blank">Others have noticed similar results for GPUs</a>, so the original kernel I wrote was a simple linear scan of the form \(h_t = \alpha_t \odot h_{t-1} + \beta_t\) where \(\alpha_t\) and \(\beta_t\) were the respective recurrent parameters, pre-computed.</p>

<p>Since I don’t have access to TPUs I re-implemented Hawk and the linear scan kernel it uses in PyTorch and Triton<sup id="fnref:2"><a href="#fn:2" class="footnote" rel="footnote" role="doc-noteref">2</a></sup>. You can view the initial linear scan kernel <a href="https://github.com/fattorib/hawk-pytorch/blob/b5ce108d89c80457ca4b36a514b57d08a6bb71a0/hawk/scan.py" rel="external nofollow noopener" target="_blank">here</a>.</p>

<p>One thing that might stand out is that before we even get to the actual linear recurrence, we are performing many elementwise operations to get the recurrent parameters ready. While its likely that XLA or <code class="language-plaintext highlighter-rouge">torch.compile</code> could fuse together some of these operations together, we can do a lot better than this by fully re-writing the kernel to use two techniques: manual <strong>operator fusion</strong> and <strong>activation recomputation</strong>.</p>

<h2 id="1-operator-fusion">1. Operator Fusion</h2>

<p>The first optimization we apply is to fuse all elementwise operations into the linear scan kernel itself. In pseudocode, our new RG-LRU computation would look as follows.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># forward pass
</span><span class="n">gate_x_no_act_BLR</span> <span class="o">=</span> <span class="n">x_BLR</span> <span class="o">@</span> <span class="n">W_xRR</span>
<span class="n">gate_a_no_act_BLR</span> <span class="o">=</span> <span class="n">x_BLR</span> <span class="o">@</span> <span class="n">W_aRR</span>

<span class="c1"># compute linear recurrence -&gt; fused into single kernel in Triton
</span><span class="n">h_t_BR</span> <span class="o">=</span> <span class="nf">zeros</span><span class="p">(</span><span class="n">B</span><span class="p">,</span><span class="n">R</span><span class="p">)</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="n">L</span><span class="p">):</span>

  <span class="c1"># load gate_x_no_act_BLR[:,i], gate_a_no_act_BLR[:,i], C_R to SRAM 
</span>  
  <span class="c1"># Start of computation in SRAM
</span>  <span class="n">gate_x_BR</span> <span class="o">=</span> <span class="nf">sigmoid</span><span class="p">(</span><span class="n">gate_x_no_act_BLR</span><span class="p">[:,</span><span class="n">i</span><span class="p">])</span>
  <span class="n">gate_a_BR</span> <span class="o">=</span> <span class="nf">sigmoid</span><span class="p">(</span><span class="n">gate_a_no_act_BLR</span><span class="p">[:,</span><span class="n">i</span><span class="p">])</span>

  <span class="n">log_a_BR</span> <span class="o">=</span> <span class="o">-</span><span class="mf">8.0</span> <span class="o">*</span> <span class="n">gate_a_BR</span> <span class="o">*</span> <span class="nf">softplus</span><span class="p">(</span><span class="n">C_R</span><span class="p">)</span>
  <span class="n">a_BR</span> <span class="o">=</span> <span class="nf">exp</span><span class="p">(</span><span class="n">log_a_BR</span><span class="p">)</span>
  <span class="n">a_square_BR</span> <span class="o">=</span> <span class="nf">exp</span><span class="p">(</span><span class="mi">2</span> <span class="o">*</span> <span class="n">log_a_BR</span><span class="p">)</span>
  <span class="n">gated_x_BR</span> <span class="o">=</span> <span class="n">x_BLR</span><span class="p">[:,</span><span class="n">i</span><span class="p">]</span> <span class="o">*</span> <span class="n">gate_x_BR</span>

  <span class="n">normalized_x_BR</span> <span class="o">=</span> <span class="n">gated_x_BR</span> <span class="o">*</span> <span class="nf">sqrt</span><span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">a_square_BR</span><span class="p">)</span>
  <span class="n">h_t_BR</span> <span class="o">=</span> <span class="n">a_BLR</span><span class="o">*</span><span class="n">h_t_BR</span> <span class="o">+</span> <span class="n">normalized_x_BR</span> 
  <span class="c1"># End of computation in SRAM
</span>  
  <span class="c1"># write h_t_BR to HBM
</span></code></pre></div></div>

<p>Assuming half-precision weights and activations, the unfused RG-LRU layer reads <code class="language-plaintext highlighter-rouge">26BLR + 4RR + 2R</code> bytes and writes <code class="language-plaintext highlighter-rouge">22BLR</code> bytes for a total of <code class="language-plaintext highlighter-rouge">48BLR + 4RR + 2R</code> bytes. The fused RG-LRU layer reads <code class="language-plaintext highlighter-rouge">4BLR + 4RR + 2R</code> bytes and writes <code class="language-plaintext highlighter-rouge">2BLR</code> bytes for a total of <code class="language-plaintext highlighter-rouge">6BLR + 4RR + 2R</code>. For all models considered, L will be greater than or equal to R and B &gt; 1. So while the memory complexity for both kernels is still $\mathcal{O}(BLR)$, the fused kernel reduces the constant factor from $48$ to $6$ leading to the fused kernel reading and writing roughly one eighth the bytes of the unfused kernel. As the runtime of this layer is memory bandwidth bound, we can expect large speedups. When implemented in Triton, this speedup over the initial kernel is large and for contexts greater than 6K tokens, we are even faster than PyTorch’s Flash Attention 2 (called via <code class="language-plaintext highlighter-rouge">scaled_dot_product_attention</code>):</p>

<div style="text-align: center">
<img src="img/scan_forward.png" alt="forward" style="width: 100%;">
</div>

<p><em>All benchmarks were performed on a single 40GB A100 from Lambda Labs with PyTorch 2.5.1 and Triton 3.1.0.</em></p>

<h2 id="2-activation-recomputation">2. Activation Recomputation</h2>

<p>While fusing essentially all the operations but the matmuls into the forward recurrence works well for inference (and really, only the first step of the inference, since this is… you know… an RNN), it is not possible to use this technique during training since we are not storing any activations needed to compute the gradients of the trainable parameters.</p>

<p>To use this fused kernel for training, we need utilize a technique called <strong>Activation Recomputation</strong> which works by “recomputing” certain intermediate activations needed for gradient computations in the backward pass instead of saving them in the forward pass, this is one of the main optimizations behind Flash Attention<sup id="fnref:3"><a href="#fn:3" class="footnote" rel="footnote" role="doc-noteref">3</a></sup>. Combining operator fusion with activation recomputation is not a new idea, while it was used for the kernels in Flash Attention, He et al.<sup id="fnref:4"><a href="#fn:4" class="footnote" rel="footnote" role="doc-noteref">4</a></sup> also show that in some more general settings, the combination of fusion and recomputation can actually speed up the forward + backward runtime of models due to the decreased memory throughput, and that selective fusion and recomputation can be determined at a graph’s compile time.</p>

<p>For the RG-LRU, this technique is simple to apply manually. Instead of viewing the linear recurrence as one big computation graph, I found it mentally easier to split up the gradient computation into two phases: the first phase computes the linear recurrence parameters:</p>

\[\begin{align*}
\alpha_t &amp;= f(r_t)\\
\beta_t &amp;= g(r_t,i_t,x_t)
\end{align*}\]

<p>Where $f$ and $g$ are computed as:</p>

\[\begin{align*}
f(r_t) &amp;= -8.0 * \sigma(r_t) * \text{softplus}(C)\\
g(r_t,i_t,x_t) &amp;= \sqrt{1-f(r_t)^2}\odot x_t \odot \sigma(i_t)
\end{align*}\]

<p>And the second phase simply computes a standard diagonal linear recurrence:</p>

\[\begin{align*}
h_t = \alpha_t \odot h_{t-1} + \beta_t
\end{align*}\]

<p>Viewing the kernel this way allows us to manually compute the gradients of $a_{t}$ and $\beta_t$ with respect to the loss<sup id="fnref:5"><a href="#fn:5" class="footnote" rel="footnote" role="doc-noteref">5</a></sup> first as:</p>

\[\begin{align*}
\nabla_{h_T} L &amp;= \frac{\partial L}{\partial h_T} \\
\nabla_{h_t} L &amp;= \frac{\partial h_{t+1}}{\partial h_t} \odot \nabla_{h_{t+1}} L + \frac{\partial L}{\partial h_t} \\
&amp;= \alpha_{t+1} \odot \nabla_{h_{t+1}} L + \frac{\partial L}{\partial h_t} \\
\nabla_{\alpha_t} L &amp;= \frac{\partial h_t}{\partial \alpha_t} \odot \nabla_{h_t} L = h_{t-1} \odot \nabla_{h_t} L \\
\nabla_{\beta_t} L &amp;= \nabla_{h_t} L \\
\nabla_{h_0} L &amp;= \frac{\partial h_1}{\partial h_0} \odot \nabla_{h_1} L = \alpha_1 \odot \nabla_{h_1} L
\end{align*}\]

<p>From here, the gradient computations for the parameters we optimize, \(x_t\), \(r_t\), \(i_t\), and \(C\) are simple elementwise gradient computations and left as an exercise 😉.</p>

<p>The forward and backward passes for this kernel were implemented in Triton and using the same benchmark setup as above, (<code class="language-plaintext highlighter-rouge">R = 1024, B = 8</code> ), our fully-fused kernel with activation recomputation is fast:</p>

<div style="text-align: center">
<img src="img/scan_forward_backward.png" alt="forward_backward" style="width: 100%;">
</div>

<p><em>All benchmarks were performed on a single 40GB A100 from Lambda Labs with PyTorch 2.5.1 and Triton 3.1.0.</em></p>

<p>Similar to the forward pass kernel, for contexts greater than 4K tokens, we are once again faster than Flash Attention 2.</p>

<h2 id="conclusion">Conclusion</h2>

<p>Using this kernel I trained a 12L/768D Hawk model on 100B tokens of OpenWebText (<em>GPT2-small config</em>):</p>

<div style="text-align: center">
<img src="img/loss.png" alt="loss" style="width: 100%;">
</div>

<p>If you’re interested in reading through the full Triton kernel it’s <a href="https://github.com/fattorib/hawk-pytorch/blob/main/hawk/scan_fused.py" rel="external nofollow noopener" target="_blank">here</a> and if you’re interested in my implementation of Hawk its <a href="https://github.com/fattorib/hawk-pytorch/tree/main" rel="external nofollow noopener" target="_blank">here</a>!</p>

<h2 id="references">References</h2>

<div class="footnotes" role="doc-endnotes">
  <ol>
    <li id="fn:1">
      <p>De, Soham, et al. “Griffin: Mixing gated linear recurrences with local attention for efficient language models.” arXiv preprint arXiv:2402.19427 (2024). <a href="#fnref:1" class="reversefootnote" role="doc-backlink">↩</a></p>
    </li>
    <li id="fn:2">
      <p>Tillet, Philippe, Hsiang-Tsung Kung, and David Cox. “Triton: an intermediate language and compiler for tiled neural network computations.” Proceedings of the 3rd ACM SIGPLAN International Workshop on Machine Learning and Programming Languages. 2019. <a href="#fnref:2" class="reversefootnote" role="doc-backlink">↩</a></p>
    </li>
    <li id="fn:3">
      <p>Dao, Tri, et al. “Flashattention: Fast and memory-efficient exact attention with io-awareness.” Advances in Neural Information Processing Systems 35 (2022): 16344-16359. <a href="#fnref:3" class="reversefootnote" role="doc-backlink">↩</a></p>
    </li>
    <li id="fn:4">
      <p>He, Horace, and Shangdi Yu. “Transcending runtime-memory tradeoffs in checkpointing by being fusion aware.” Proceedings of Machine Learning and Systems 5 (2023): 414-427. <a href="#fnref:4" class="reversefootnote" role="doc-backlink">↩</a></p>
    </li>
    <li id="fn:5">
      <p>Martin, Eric, and Chris Cundy. “Parallelizing linear recurrent neural nets over sequence length.” arXiv preprint arXiv:1709.04057 (2017). <a href="#fnref:5" class="reversefootnote" role="doc-backlink">↩</a></p>
    </li>
  </ol>
</div>

      </d-article>

      <d-appendix>
        <d-footnote-list></d-footnote-list>
        <d-citation-list></d-citation-list>
      </d-appendix>

      <d-bibliography src="/assets/bibliography/"></d-bibliography>
</div>

    <!-- Footer -->
<footer class="fixed-bottom">
  <div class="container mt-0">
    All content licensed under <a href="https://creativecommons.org/licenses/by/4.0/" rel="external nofollow noopener" target="_blank">CC-BY 4.0</a> 2024 Benjamin  Fattori. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>.
Last updated: December 26, 2024.
  </div>
</footer>

    <!-- Bootsrap & MDB scripts -->
  <script src="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/js/bootstrap.bundle.min.js" integrity="sha256-fgLAgv7fyCGopR/gBNq2iW3ZKIdqIcyshnUULC4vex8=" crossorigin="anonymous"></script>
  <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script>

    
    

<!-- Scrolling Progress Bar -->
<script type="text/javascript">
  /*
   * This JavaScript code has been adapted from the article 
   * https://css-tricks.com/reading-position-indicator/ authored by Pankaj Parashar, 
   * published on the website https://css-tricks.com on the 7th of May, 2014.
   * Couple of changes were made to the original code to make it compatible 
   * with the `al-foio` theme.
   */
  const progressBar = $("#progress");
  /*
   * We set up the bar after all elements are done loading.
   * In some cases, if the images in the page are larger than the intended
   * size they'll have on the page, they'll be resized via CSS to accomodate
   * the desired size. This mistake, however, breaks the computations as the
   * scroll size is computed as soon as the elements finish loading.
   * To account for this, a minimal delay was introduced before computing the
   * values.
   */
  window.onload = function () {
    setTimeout(progressBarSetup, 50);
  };
  /*
   * We set up the bar according to the browser.
   * If the browser supports the progress element we use that.
   * Otherwise, we resize the bar thru CSS styling
   */
  function progressBarSetup() {
    if ("max" in document.createElement("progress")) {
      initializeProgressElement();
      $(document).on("scroll", function() {
        progressBar.attr({ value: getCurrentScrollPosition() });
      });
      $(window).on("resize", initializeProgressElement);
    } else {
      resizeProgressBar();
      $(document).on("scroll", resizeProgressBar);
      $(window).on("resize", resizeProgressBar);
    }
  }
  /*
   * The vertical scroll position is the same as the number of pixels that
   * are hidden from view above the scrollable area. Thus, a value > 0 is
   * how much the user has scrolled from the top
   */
  function getCurrentScrollPosition() {
    return $(window).scrollTop();
  }

  function initializeProgressElement() {
    let navbarHeight = $("#navbar").outerHeight(true);
    $("body").css({ "padding-top": navbarHeight });
    $("progress-container").css({ "padding-top": navbarHeight });
    progressBar.css({ top: navbarHeight });
    progressBar.attr({
      max: getDistanceToScroll(),
      value: getCurrentScrollPosition(),
    });
  }
  /*
   * The offset between the html document height and the browser viewport
   * height will be greater than zero if vertical scroll is possible.
   * This is the distance the user can scroll
   */
  function getDistanceToScroll() {
    return $(document).height() - $(window).height();
  }

  function resizeProgressBar() {
    progressBar.css({ width: getWidthPercentage() + "%" });
  }
  // The scroll ratio equals the percentage to resize the bar
  function getWidthPercentage() {
    return (getCurrentScrollPosition() / getDistanceToScroll()) * 100;
  }
</script>

  
</body>
</html>
