<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.3.4">Jekyll</generator><link href="https://fattorib.github.io/feed.xml" rel="self" type="application/atom+xml" /><link href="https://fattorib.github.io/" rel="alternate" type="text/html" hreflang="en" /><updated>2024-12-26T14:37:01+00:00</updated><id>https://fattorib.github.io/feed.xml</id><title type="html">blank</title><subtitle>A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design.
</subtitle><entry><title type="html">A fast RG-LRU Kernel</title><link href="https://fattorib.github.io/blog/2024/rg-lru/" rel="alternate" type="text/html" title="A fast RG-LRU Kernel" /><published>2024-12-26T04:00:00+00:00</published><updated>2024-12-26T04:00:00+00:00</updated><id>https://fattorib.github.io/blog/2024/rg-lru</id><content type="html" xml:base="https://fattorib.github.io/blog/2024/rg-lru/"><![CDATA[<h1 id="a-fast-rg-lru-kernel">A fast RG-LRU Kernel</h1>

<p><strong>TL;DR:</strong> I develop a IO-aware kernel for the RG-LRU in Triton. When training on sequences greater than 4K tokens, the kernel is faster than PyTorch’s Flash Attention 2 implementation.</p>

<h2 id="introduction">Introduction</h2>

<p>Hawk is an RNN proposed by Google Deepmind in “<strong>Griffin: Mixing Gated Linear Recurrences with Local Attention for Efficient Language Models”.</strong><sup id="fnref:1"><a href="#fn:1" class="footnote" rel="footnote" role="doc-noteref">1</a></sup> At its core is the Real-Gated Linear Recurrent Unit (RG-LRU) layer, a <em>linear</em> and <em>diagonal</em> recurrence. A linear and diagonal recurrence means each element in the hidden state only depends on its corresponding element from the previous timestep.</p>

<p>The equations for computing the recurrence are:</p>

\[\begin{align*}
r_t &amp;= \sigma(W_a x_t + b_a) \\
i_t &amp;= \sigma(W_x x_t + b_x) \\
a_t &amp;= a^{C r_t} \\
h_t &amp;= a_t \odot h_{t-1} + \sqrt{1 -  a_{t}^2} \odot (i_t \odot x_t)
\end{align*}\]

<p>Where \(x_t\) is of shape <code class="language-plaintext highlighter-rouge">[B, L, R]</code> , \(h_t\) is of shape <code class="language-plaintext highlighter-rouge">[B, R]</code> and \(C\) is a trainable parameter of size <code class="language-plaintext highlighter-rouge">R</code>. In PyTorch psuedocode, the forward pass is</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="sh">"""</span><span class="s">Example RG-LRU forward pass:
B: batch size
L: sequence length 
D: model dimension
R: Recurrent dimension
</span><span class="sh">"""</span>

<span class="c1"># forward pass
</span><span class="n">gate_x_BLR</span> <span class="o">=</span> <span class="nf">sigmoid</span><span class="p">(</span><span class="n">x_BLR</span> <span class="o">@</span> <span class="n">W_xRR</span><span class="p">)</span>
<span class="n">gate_a_BLR</span> <span class="o">=</span> <span class="nf">sigmoid</span><span class="p">(</span><span class="n">x_BLR</span> <span class="o">@</span> <span class="n">W_aRR</span><span class="p">)</span>

<span class="n">log_a_BLR</span> <span class="o">=</span> <span class="o">-</span><span class="mf">8.0</span> <span class="o">*</span> <span class="n">gate_a_BLR</span> <span class="o">*</span> <span class="nf">softplus</span><span class="p">(</span><span class="n">C_R</span><span class="p">)</span>
<span class="n">a_BLR</span> <span class="o">=</span> <span class="nf">exp</span><span class="p">(</span><span class="n">log_a_BLR</span><span class="p">)</span>
<span class="n">a_square_BLR</span> <span class="o">=</span> <span class="nf">exp</span><span class="p">(</span><span class="mi">2</span> <span class="o">*</span> <span class="n">log_a_BLR</span><span class="p">)</span>
<span class="n">gated_x_BLR</span> <span class="o">=</span> <span class="n">x_BLR</span> <span class="o">*</span> <span class="n">gate_x_BLR</span>

<span class="n">normalized_x_BLR</span> <span class="o">=</span> <span class="n">gated_x_BLR</span> <span class="o">*</span> <span class="nf">sqrt</span><span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">a_square_BLR</span><span class="p">)</span>

<span class="c1"># compute linear recurrence -&gt; fused into single kernel in Pallas or Triton
</span><span class="n">h_t_BR</span> <span class="o">=</span> <span class="nf">zeros</span><span class="p">(</span><span class="n">B</span><span class="p">,</span><span class="n">R</span><span class="p">)</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="n">L</span><span class="p">):</span>
  <span class="c1"># load a_BLR[:,i] and normalized_x_BLR[:,i] to SRAM
</span>  <span class="n">h_t_BR</span> <span class="o">=</span> <span class="n">a_BLR</span><span class="p">[:,</span><span class="n">i</span><span class="p">]</span><span class="o">*</span><span class="n">h_t_BR</span> <span class="o">+</span> <span class="n">normalized_x_BLR</span><span class="p">[:,</span><span class="n">i</span><span class="p">]</span> 
  <span class="c1"># write h_t_BR to HBM
</span></code></pre></div></div>

<p>As the recurrence itself performs no matrix-multiplications and only elementwise operations, it follows that the runtime of this kernel will be memory-bound on any GPU or TPU. The authors develop a custom Pallas kernel to compute this linear recurrence, which can be viewed on DM’s “RecurrentGemma” Github <a href="https://github.com/google-deepmind/recurrentgemma/blob/main/recurrentgemma/jax/pallas.py#L475">here</a>. On the systems side, an interesting takeaway from this paper is that a “naive” fused linear scan kernel actually outperforms the more parallelizeable associative scan kernel. <a href="https://x.com/SonglinYang4/status/1763598848130453728">Others have noticed similar results for GPUs</a>, so the original kernel I wrote was a simple linear scan of the form \(h_t = \alpha_t \odot h_{t-1} + \beta_t\) where \(\alpha_t\) and \(\beta_t\) were the respective recurrent parameters, pre-computed.</p>

<p>Since I don’t have access to TPUs I re-implemented Hawk and the linear scan kernel it uses in PyTorch and Triton<sup id="fnref:2"><a href="#fn:2" class="footnote" rel="footnote" role="doc-noteref">2</a></sup>. You can view the initial linear scan kernel <a href="https://github.com/fattorib/hawk-pytorch/blob/b5ce108d89c80457ca4b36a514b57d08a6bb71a0/hawk/scan.py">here</a>.</p>

<p>One thing that might stand out is that before we even get to the actual linear recurrence, we are performing many elementwise operations to get the recurrent parameters ready. While its likely that XLA or <code class="language-plaintext highlighter-rouge">torch.compile</code> could fuse together some of these operations together, we can do a lot better than this by fully re-writing the kernel to use two techniques: manual <strong>operator fusion</strong> and <strong>activation recomputation</strong>.</p>

<h2 id="1-operator-fusion">1. Operator Fusion</h2>

<p>The first optimization we apply is to fuse all elementwise operations into the linear scan kernel itself. In pseudocode, our new RG-LRU computation would look as follows.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># forward pass
</span><span class="n">gate_x_no_act_BLR</span> <span class="o">=</span> <span class="n">x_BLR</span> <span class="o">@</span> <span class="n">W_xRR</span>
<span class="n">gate_a_no_act_BLR</span> <span class="o">=</span> <span class="n">x_BLR</span> <span class="o">@</span> <span class="n">W_aRR</span>

<span class="c1"># compute linear recurrence -&gt; fused into single kernel in Triton
</span><span class="n">h_t_BR</span> <span class="o">=</span> <span class="nf">zeros</span><span class="p">(</span><span class="n">B</span><span class="p">,</span><span class="n">R</span><span class="p">)</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="n">L</span><span class="p">):</span>

  <span class="c1"># load gate_x_no_act_BLR[:,i], gate_a_no_act_BLR[:,i], C_R to SRAM 
</span>  
  <span class="c1"># Start of computation in SRAM
</span>  <span class="n">gate_x_BR</span> <span class="o">=</span> <span class="nf">sigmoid</span><span class="p">(</span><span class="n">gate_x_no_act_BLR</span><span class="p">[:,</span><span class="n">i</span><span class="p">])</span>
  <span class="n">gate_a_BR</span> <span class="o">=</span> <span class="nf">sigmoid</span><span class="p">(</span><span class="n">gate_a_no_act_BLR</span><span class="p">[:,</span><span class="n">i</span><span class="p">])</span>

  <span class="n">log_a_BR</span> <span class="o">=</span> <span class="o">-</span><span class="mf">8.0</span> <span class="o">*</span> <span class="n">gate_a_BR</span> <span class="o">*</span> <span class="nf">softplus</span><span class="p">(</span><span class="n">C_R</span><span class="p">)</span>
  <span class="n">a_BR</span> <span class="o">=</span> <span class="nf">exp</span><span class="p">(</span><span class="n">log_a_BR</span><span class="p">)</span>
  <span class="n">a_square_BR</span> <span class="o">=</span> <span class="nf">exp</span><span class="p">(</span><span class="mi">2</span> <span class="o">*</span> <span class="n">log_a_BR</span><span class="p">)</span>
  <span class="n">gated_x_BR</span> <span class="o">=</span> <span class="n">x_BLR</span><span class="p">[:,</span><span class="n">i</span><span class="p">]</span> <span class="o">*</span> <span class="n">gate_x_BR</span>

  <span class="n">normalized_x_BR</span> <span class="o">=</span> <span class="n">gated_x_BR</span> <span class="o">*</span> <span class="nf">sqrt</span><span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">a_square_BR</span><span class="p">)</span>
  <span class="n">h_t_BR</span> <span class="o">=</span> <span class="n">a_BLR</span><span class="o">*</span><span class="n">h_t_BR</span> <span class="o">+</span> <span class="n">normalized_x_BR</span> 
  <span class="c1"># End of computation in SRAM
</span>  
  <span class="c1"># write h_t_BR to HBM
</span></code></pre></div></div>

<p>Assuming half-precision weights and activations, the unfused RG-LRU layer reads <code class="language-plaintext highlighter-rouge">26BLR + 4RR + 2R</code> bytes and writes <code class="language-plaintext highlighter-rouge">22BLR</code> bytes for a total of <code class="language-plaintext highlighter-rouge">48BLR + 4RR + 2R</code> bytes. The fused RG-LRU layer reads <code class="language-plaintext highlighter-rouge">4BLR + 4RR + 2R</code> bytes and writes <code class="language-plaintext highlighter-rouge">2BLR</code> bytes for a total of <code class="language-plaintext highlighter-rouge">6BLR + 4RR + 2R</code>. For all models considered, L will be greater than or equal to R and B &gt; 1. So while the memory complexity for both kernels is still $\mathcal{O}(BLR)$, the fused kernel reduces the constant factor from $48$ to $6$ leading to the fused kernel reading and writing roughly one eighth the bytes of the unfused kernel. As the runtime of this layer is memory bandwidth bound, we can expect large speedups. When implemented in Triton, this speedup over the initial kernel is large and for contexts greater than 6K tokens, we are even faster than PyTorch’s Flash Attention 2 (called via <code class="language-plaintext highlighter-rouge">scaled_dot_product_attention</code>):</p>

<div style="text-align: center">
<img src="img/scan_forward.png" alt="forward" style="width: 100%;" />
</div>

<p><em>All benchmarks were performed on a single 40GB A100 from Lambda Labs with PyTorch 2.5.1 and Triton 3.1.0.</em></p>

<h2 id="2-activation-recomputation">2. Activation Recomputation</h2>

<p>While fusing essentially all the operations but the matmuls into the forward recurrence works well for inference (and really, only the first step of the inference, since this is… you know… an RNN), it is not possible to use this technique during training since we are not storing any activations needed to compute the gradients of the trainable parameters.</p>

<p>To use this fused kernel for training, we need utilize a technique called <strong>Activation Recomputation</strong> which works by “recomputing” certain intermediate activations needed for gradient computations in the backward pass instead of saving them in the forward pass, this is one of the main optimizations behind Flash Attention<sup id="fnref:3"><a href="#fn:3" class="footnote" rel="footnote" role="doc-noteref">3</a></sup>. Combining operator fusion with activation recomputation is not a new idea, while it was used for the kernels in Flash Attention, He et al.<sup id="fnref:4"><a href="#fn:4" class="footnote" rel="footnote" role="doc-noteref">4</a></sup> also show that in some more general settings, the combination of fusion and recomputation can actually speed up the forward + backward runtime of models due to the decreased memory throughput, and that selective fusion and recomputation can be determined at a graph’s compile time.</p>

<p>For the RG-LRU, this technique is simple to apply manually. Instead of viewing the linear recurrence as one big computation graph, I found it mentally easier to split up the gradient computation into two phases: the first phase computes the linear recurrence parameters:</p>

\[\begin{align*}
\alpha_t &amp;= f(r_t)\\
\beta_t &amp;= g(r_t,i_t,x_t)
\end{align*}\]

<p>Where $f$ and $g$ are computed as:</p>

\[\begin{align*}
f(r_t) &amp;= -8.0 * \sigma(r_t) * \text{softplus}(C)\\
g(r_t,i_t,x_t) &amp;= \sqrt{1-f(r_t)^2}\odot x_t \odot \sigma(i_t)
\end{align*}\]

<p>And the second phase simply computes a standard diagonal linear recurrence:</p>

\[\begin{align*}
h_t = \alpha_t \odot h_{t-1} + \beta_t
\end{align*}\]

<p>Viewing the kernel this way allows us to manually compute the gradients of $a_{t}$ and $\beta_t$ with respect to the loss<sup id="fnref:5"><a href="#fn:5" class="footnote" rel="footnote" role="doc-noteref">5</a></sup> first as:</p>

\[\begin{align*}
\nabla_{h_T} L &amp;= \frac{\partial L}{\partial h_T} \\
\nabla_{h_t} L &amp;= \frac{\partial h_{t+1}}{\partial h_t} \odot \nabla_{h_{t+1}} L + \frac{\partial L}{\partial h_t} \\
&amp;= \alpha_{t+1} \odot \nabla_{h_{t+1}} L + \frac{\partial L}{\partial h_t} \\
\nabla_{\alpha_t} L &amp;= \frac{\partial h_t}{\partial \alpha_t} \odot \nabla_{h_t} L = h_{t-1} \odot \nabla_{h_t} L \\
\nabla_{\beta_t} L &amp;= \nabla_{h_t} L \\
\nabla_{h_0} L &amp;= \frac{\partial h_1}{\partial h_0} \odot \nabla_{h_1} L = \alpha_1 \odot \nabla_{h_1} L
\end{align*}\]

<p>From here, the gradient computations for the parameters we optimize, \(x_t\), \(r_t\), \(i_t\), and \(C\) are simple elementwise gradient computations and left as an exercise 😉.</p>

<p>The forward and backward passes for this kernel were implemented in Triton and using the same benchmark setup as above, (<code class="language-plaintext highlighter-rouge">R = 1024, B = 8</code> ), our fully-fused kernel with activation recomputation is fast:</p>

<div style="text-align: center">
<img src="img/scan_forward_backward.png" alt="forward_backward" style="width: 100%;" />
</div>

<p><em>All benchmarks were performed on a single 40GB A100 from Lambda Labs with PyTorch 2.5.1 and Triton 3.1.0.</em></p>

<p>Similar to the forward pass kernel, for contexts greater than 4K tokens, we are once again faster than Flash Attention 2.</p>

<h2 id="conclusion">Conclusion</h2>

<p>Using this kernel I trained a 12L/768D Hawk model on 100B tokens of OpenWebText (<em>GPT2-small config</em>):</p>

<div style="text-align: center">
<img src="img/loss.png" alt="loss" style="width: 100%;" />
</div>

<p>If you’re interested in reading through the full Triton kernel it’s <a href="https://github.com/fattorib/hawk-pytorch/blob/main/hawk/scan_fused.py">here</a> and if you’re interested in my implementation of Hawk its <a href="https://github.com/fattorib/hawk-pytorch/tree/main">here</a>!</p>

<h2 id="references">References</h2>

<div class="footnotes" role="doc-endnotes">
  <ol>
    <li id="fn:1">
      <p>De, Soham, et al. “Griffin: Mixing gated linear recurrences with local attention for efficient language models.” arXiv preprint arXiv:2402.19427 (2024). <a href="#fnref:1" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:2">
      <p>Tillet, Philippe, Hsiang-Tsung Kung, and David Cox. “Triton: an intermediate language and compiler for tiled neural network computations.” Proceedings of the 3rd ACM SIGPLAN International Workshop on Machine Learning and Programming Languages. 2019. <a href="#fnref:2" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:3">
      <p>Dao, Tri, et al. “Flashattention: Fast and memory-efficient exact attention with io-awareness.” Advances in Neural Information Processing Systems 35 (2022): 16344-16359. <a href="#fnref:3" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:4">
      <p>He, Horace, and Shangdi Yu. “Transcending runtime-memory tradeoffs in checkpointing by being fusion aware.” Proceedings of Machine Learning and Systems 5 (2023): 414-427. <a href="#fnref:4" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:5">
      <p>Martin, Eric, and Chris Cundy. “Parallelizing linear recurrent neural nets over sequence length.” arXiv preprint arXiv:1709.04057 (2017). <a href="#fnref:5" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
  </ol>
</div>]]></content><author><name>Benjamin Fattori</name></author><category term="Triton" /><summary type="html"><![CDATA[A fast RG-LRU Kernel]]></summary></entry><entry><title type="html">ZeRO Optimizer Sharding with xmap and pjit</title><link href="https://fattorib.github.io/blog/2023/jax-pmap/" rel="alternate" type="text/html" title="ZeRO Optimizer Sharding with xmap and pjit" /><published>2023-03-31T15:59:00+00:00</published><updated>2023-03-31T15:59:00+00:00</updated><id>https://fattorib.github.io/blog/2023/jax-pmap</id><content type="html" xml:base="https://fattorib.github.io/blog/2023/jax-pmap/"><![CDATA[<p><strong>TL;DR</strong> I improve upon my earlier codebase by implementing ZeRO-1 optimizer sharding using a combination of <code class="language-plaintext highlighter-rouge">xmap</code> and <code class="language-plaintext highlighter-rouge">pjit</code>. The resultant method is more performant and scales better across multiple TPU hosts achieving 67% MFU on a TPU v3-32. I use this method to train a 1.3B parameter decoder-only transformer on 200B tokens. The codebase I wrote can be found <a href="https://github.com/fattorib/ZeRO-transformer">here</a>.</p>

<h2 id="background">Background</h2>

<p>For the past 4 months, I have been spending time learning more about jax and the parallelism APIs it offers. The first large project I completed was an implementation of <a href="https://arxiv.org/abs/1910.02054">ZeRO-1</a> optimizer sharding using the <code class="language-plaintext highlighter-rouge">pmap</code> operator. Using this, I was able to train a 1.1B parameter decoder-only transformer on a TPU v3-32, by using 8-way optimizer state sharding on each of the TPU hosts.</p>

<p>While this was successful, using <code class="language-plaintext highlighter-rouge">pmap</code> requires a lot of manual array handling to ensure that everything passed into a pmapped function can be distributed across the local devices. In my code, this resulted in multiple helper functions solely for sharding/unsharding arrays. In retrospect, these functions were more of a performance bottleneck than I had originally expected. In addition, without some complicated, and error-prone, communication code, extending the optimizer sharding across hosts would be very difficult<sup id="fnref:1"><a href="#fn:1" class="footnote" rel="footnote" role="doc-noteref">1</a></sup>.</p>

<h2 id="zero-optimizer-sharding-overview">ZeRO Optimizer Sharding Overview</h2>

<p>ZeRO, introduced in the paper <a href="https://arxiv.org/abs/1910.02054">ZeRO: Memory Optimizations Toward Training Trillion Parameter Models</a> by Rajbhandari et al. stands for Zero Redundancy Optimizer. The authors of the paper propose a modification to training using data parallelism (splitting input batches across all available devices) that reduces memory usage by splitting the buffers of the model’s optimizer across all the data parallel ranks. With a proper communication implementation, they are able to retain the high throughput of data parallel training while training larger models than were previously possible. The original paper introduces multiple levels of ZeRO:</p>
<ul>
  <li>ZeRO-1 shards the optimizer states</li>
  <li>ZeRO-2 also shards the gradients as well</li>
  <li>ZeRO-DP shards the model parameters themselves across data parallel ranks, which is similar to <a href="https://engineering.fb.com/2021/07/15/open-source/fsdp/">FullyShardedDataParallel</a></li>
</ul>

<h2 id="going-forward---pjit-everything">Going forward - pjit everything?</h2>

<p>I recently got access to some more TPU compute and wanted to extend my previous code to address some of the pain points mentioned above. Originally, I had planned to only use pjit to accomplish this. In theory, this is easy to do: we specify the <code class="language-plaintext highlighter-rouge">PartitionSpec</code> for the optimizer pytree, duplicate that for the gradient pytree, ensure the batches are split across the same axis and then we’re good to go!</p>

<p>In practice, however, I found that naively using pjit this way resulted in significant throughput decreases. I suspect that the pjit-compilied function was forcing a gradient all-reduce every accumulation step, instead of once every batch, decreasing the training throughput by roughly a factor of <code class="language-plaintext highlighter-rouge">gradient_accumulation_steps</code>.</p>

<p><em>However, I have been unable to alleviate this issue even after adding in proper sharding annotations with <code class="language-plaintext highlighter-rouge">with_sharding_constraint</code>, so I suspect that I still have a mistake in my code somewhere. I’m currently going over how <a href="https://github.com/salesforce/jaxformer">jaxformer</a> implements their gradient accumulation code and am hopeful following how <a href="https://github.com/salesforce/jaxformer/blob/main/jaxformer/models/decoder/inter/model.py#L214">they do</a> it will work.</em></p>

<h2 id="xmap-to-the-rescue">xmap to the rescue!</h2>

<p>Thankfully, there is a second jax parallelism API that is intended to shard arrays across multiple hosts: <code class="language-plaintext highlighter-rouge">xmap</code>. The main between <code class="language-plaintext highlighter-rouge">xmap</code> and <code class="language-plaintext highlighter-rouge">pjit</code> that we are interested in is that <code class="language-plaintext highlighter-rouge">xmap</code> compiled code still requires the user to specify how and when collective operations (ex: <code class="language-plaintext highlighter-rouge">pmean</code>, <code class="language-plaintext highlighter-rouge">psum</code>, <code class="language-plaintext highlighter-rouge">all_gather</code>) occur, unlike <code class="language-plaintext highlighter-rouge">pjit</code> which will automatically insert these whenever the compiler deems necessary. From this perspective, <code class="language-plaintext highlighter-rouge">xmap</code> can be seen as a generalization of <code class="language-plaintext highlighter-rouge">pmap</code>, which also requires that users specify when they want collective operations to be applied.</p>

<p>One nice upgrade that <code class="language-plaintext highlighter-rouge">xmap</code> has over <code class="language-plaintext highlighter-rouge">pmap</code> is its reliance on named axes. By specifying a list of named axes to xmap, we can control how inputs and outputs to an xmapped function are sharded as well as how and when operations such as <code class="language-plaintext highlighter-rouge">pmean</code> are applied. Most importantly, xmapped functions compose properly with pjitted functions, something I was not able to get working with <code class="language-plaintext highlighter-rouge">pmap</code> and <code class="language-plaintext highlighter-rouge">pjit</code>.</p>

<h2 id="putting-it-all-together-xmap--pjit">Putting it all together: xmap &amp; pjit</h2>

<p>Because xmap and pjit arrays are of the same type, it is possible to pass the output from an xmapped function into a pjitted function. The communication pattern for the gradient accumulation code is straightforward: we iterate over <code class="language-plaintext highlighter-rouge">gradient_accumulation_steps</code> microbatches and have every TPU core compute its own local set of gradients, once that is completed, we use a single <code class="language-plaintext highlighter-rouge">pmean</code> to synchronize gradients across all TPU cores and we’re done!</p>

<p>From here, we can take these output gradients and shard them to match the <code class="language-plaintext highlighter-rouge">PartitionSpec</code> of the optimizer states. Remember from above, we’ve distributed these across all available TPU cores the easiest way to do this is to just use <code class="language-plaintext highlighter-rouge">ParitionSpec('dp', None)</code> or <code class="language-plaintext highlighter-rouge">ParitionSpec('dp')</code> to shard the first axis of all weights across the data parallel axis, in this case denoted by <code class="language-plaintext highlighter-rouge">'dp'</code>. In the end, the resulting code is compact and easy-to-follow:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">in_axes</span> <span class="o">=</span><span class="p">(</span>
    <span class="p">[...],</span> 
    <span class="p">[</span><span class="sh">'</span><span class="s">batch</span><span class="sh">'</span><span class="p">,</span> <span class="p">...],</span> 
    <span class="p">[...],</span> 
    <span class="p">)</span>

<span class="n">out_axes</span> <span class="o">=</span> <span class="p">(</span>
    <span class="p">[...],</span>
    <span class="p">[...]</span>
<span class="p">)</span>
<span class="c1"># compute gradients with standard data-parallel
</span><span class="n">grads</span><span class="p">,</span> <span class="n">metrics</span> <span class="o">=</span> <span class="nf">xmap</span><span class="p">(</span>
    <span class="nf">partial</span><span class="p">(</span><span class="n">train_step</span><span class="p">,</span> <span class="n">model</span> <span class="o">=</span> <span class="n">model</span><span class="p">,</span> <span class="n">accum_steps</span> <span class="o">=</span> <span class="n">GRAD_ACCUM_STEPS</span><span class="p">),</span>
    <span class="n">in_axes</span><span class="o">=</span><span class="n">in_axes</span><span class="p">,</span>
    <span class="n">out_axes</span><span class="o">=</span><span class="n">out_axes</span><span class="p">,</span>
    <span class="n">axis_resources</span><span class="o">=</span><span class="p">{</span><span class="sh">"</span><span class="s">batch</span><span class="sh">"</span><span class="p">:</span> <span class="sh">"</span><span class="s">dp</span><span class="sh">"</span><span class="p">}</span>
<span class="p">)(</span><span class="n">params</span><span class="p">,</span> <span class="n">batch</span><span class="p">,</span> <span class="n">dropout_rng</span><span class="p">)</span>   

<span class="n">grads</span> <span class="o">=</span> <span class="nf">pjit</span><span class="p">(</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span><span class="n">x</span><span class="p">,</span> <span class="n">in_axis_resources</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span> <span class="n">out_axis_resources</span><span class="o">=</span><span class="n">grad_param_spec</span><span class="p">)(</span><span class="n">grads</span><span class="p">)</span>
<span class="n">params</span> <span class="o">=</span> <span class="nf">pjit</span><span class="p">(</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span><span class="n">x</span><span class="p">,</span> <span class="n">in_axis_resources</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span> <span class="n">out_axis_resources</span><span class="o">=</span><span class="n">grad_param_spec</span><span class="p">)(</span><span class="n">params</span><span class="p">)</span>

<span class="c1"># each dp process updates their own copy of the optimizer state before updating params and 
# performing an all-gather to sync params
</span><span class="n">new_params</span><span class="p">,</span><span class="n">new_opt_state</span> <span class="o">=</span> <span class="nf">pjit</span><span class="p">(</span>
    <span class="n">functools</span><span class="p">.</span><span class="nf">partial</span><span class="p">(</span><span class="n">update_opt_state</span><span class="p">,</span> <span class="n">optimizer</span> <span class="o">=</span> <span class="n">tx</span><span class="p">,</span> <span class="n">grad_spec</span> <span class="o">=</span> <span class="n">grad_param_spec</span><span class="p">),</span>
    <span class="n">in_axis_resources</span><span class="o">=</span><span class="p">(</span><span class="n">grad_param_spec</span><span class="p">,</span> <span class="n">opt_state_spec</span><span class="p">,</span> <span class="n">grad_param_spec</span><span class="p">),</span>
    <span class="n">out_axis_resources</span><span class="o">=</span><span class="p">(</span><span class="bp">None</span><span class="p">,</span><span class="n">opt_state_spec</span><span class="p">),</span>
<span class="p">)(</span><span class="n">grads</span><span class="p">,</span> <span class="n">opt_state</span><span class="p">,</span> <span class="n">params</span><span class="p">)</span>
</code></pre></div></div>

<p>In comparison to my original code, this is much more simple. Specifying proper in/out axis resources ensures I don’t need to write any pytree reshaping code which I found eats up device HBM and causes slowdowns. The only manual sharding we do is the pjitted identity functions to shard the gradients and paramaters to the correct processes for the optimizer update.</p>

<h2 id="performance-benchmarks">Performance Benchmarks</h2>

<p>On a TPU v3-32, training a 1.3B parameter model with BF16 mixed-precision, a sequence length of 1024 tokens and a global batch size of 512, the code processes one batch every 1.65 seconds. This converts to 2.638 PFLOP/s or 67% Model FLOPs Utilization (MFU). (TPU Max TFLOPs calculated by multiplying <a href="https://cloud.google.com/tpu/docs/system-architecture-tpu-vm#tpu_v3">peak compute per chip</a> by 32.)</p>

<h2 id="training-a-13b-parameter-decoder-only-transformer">Training a 1.3B Parameter Decoder-only Transformer</h2>

<p>To prove out that my implementation works, I trained a 1.3B parameter language model on 200B tokens from <a href="https://pile.eleuther.ai/">The Pile</a>. Naively, this model requires more than ~17 GiB to store the params and optimizer states alone, which is already more than the 16.0 GiB of memory each TPU V3 core has access to.</p>

<p>I selected The Pile as my training dataset as it is a large and diverse corpus of text that is easily available. To save on storage and processing costs, I only trained for multiple epochs on a small subset of the pile, specifically <code class="language-plaintext highlighter-rouge">00.json.zst</code> to <code class="language-plaintext highlighter-rouge">03.jsonl.zst</code> from <a href="https://the-eye.eu/public/AI/pile/train/">here</a>. The total dataset was shuffled prior to being split into these files, so these slices are still a representative sample of the data. The text was tokenized using the Byte-Level <a href="https://huggingface.co/docs/transformers/model_doc/gpt_neox#transformers.GPTNeoXTokenizerFast">GPTNeoX tokenizer</a>. Sequences were tokenized and an end-of-text token was appended to the end of documents. The total dataset consists of approximately 30B tokens. The pile validation file <a href="https://the-eye.eu/public/AI/pile/">here</a> was used for the validation set.</p>

<p>The training code was implemented in <a href="https://github.com/google/flax">Flax</a> with optimizers from <a href="https://github.com/deepmind/optax">Optax</a>. For reproducibility, the following version of all packages used were:</p>
<details open="">
```bash
flax==0.6.3, optax==0.1.3, jax[tpu]==0.4.6
```
</details>

<h3 id="model">Model</h3>

<table>
  <thead>
    <tr>
      <th>Hyperparameter</th>
      <th>Value</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>n_parameters</td>
      <td>1.3B</td>
    </tr>
    <tr>
      <td>n_layers</td>
      <td>24</td>
    </tr>
    <tr>
      <td>d_model</td>
      <td>2048</td>
    </tr>
    <tr>
      <td>d_ff</td>
      <td>8192</td>
    </tr>
    <tr>
      <td>num_head</td>
      <td>16</td>
    </tr>
    <tr>
      <td>d_head</td>
      <td>128</td>
    </tr>
    <tr>
      <td>vocab_size</td>
      <td>50304</td>
    </tr>
    <tr>
      <td>Positional Encoding</td>
      <td>ALiBi</td>
    </tr>
    <tr>
      <td>n_ctx</td>
      <td>1024</td>
    </tr>
  </tbody>
</table>

<h3 id="training">Training</h3>

<p>The model was trained for 200B tokens with a batch size of ~0.5M tokens using the following hyperparameters:</p>

<table>
  <thead>
    <tr>
      <th>Hyperparameter</th>
      <th>Value</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>Batch Size</td>
      <td>0.5M Tokens</td>
    </tr>
    <tr>
      <td>Peak Learning Rate</td>
      <td>2.0e-4</td>
    </tr>
    <tr>
      <td>Warmup Steps</td>
      <td>2000</td>
    </tr>
    <tr>
      <td>Residual Dropout</td>
      <td>0.1</td>
    </tr>
    <tr>
      <td>Attention Dropout</td>
      <td>0.1</td>
    </tr>
    <tr>
      <td>Embedding Dropout</td>
      <td>0.0</td>
    </tr>
    <tr>
      <td>Precision</td>
      <td>bfloat16</td>
    </tr>
    <tr>
      <td>Weight Decay</td>
      <td>0.1</td>
    </tr>
    <tr>
      <td>Optimizer</td>
      <td>AdamW</td>
    </tr>
    <tr>
      <td>Schedule</td>
      <td>Cosine decay to 2.0e-5</td>
    </tr>
  </tbody>
</table>

<h3 id="benchmarks">Benchmarks</h3>

<p>Benchmarks are performed using my fork of <a href="https://github.com/fattorib/lm-evaluation-harness">lm-evaluation-harness</a>:</p>

<table>
  <thead>
    <tr>
      <th>Model</th>
      <th>LAMBADA PPL</th>
      <th>LAMBADA Acc</th>
      <th>Wikitext BPB</th>
      <th>PIQA Acc</th>
      <th>Hellaswag Acc Norm</th>
      <th>Winogrande Acc</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><strong>GPT-1.3B (Mine)</strong></td>
      <td><strong>7.69</strong></td>
      <td><strong>57.15%</strong></td>
      <td><strong>0.8158</strong></td>
      <td><strong>69.48%</strong></td>
      <td><strong>45.2%</strong></td>
      <td><strong>55.09%</strong></td>
    </tr>
    <tr>
      <td>GPT-Neo 1.3B</td>
      <td>7.498</td>
      <td>57.23%</td>
      <td>-</td>
      <td>71.11%</td>
      <td>48.93%</td>
      <td>55.01%</td>
    </tr>
    <tr>
      <td>GPT-3 1.3B</td>
      <td>5.44</td>
      <td>63.6%</td>
      <td>-</td>
      <td>75.1%</td>
      <td>54.7%</td>
      <td>58.7%</td>
    </tr>
  </tbody>
</table>

<p>The final validation loss on The Pile is 2.206, which works out to a bits-per-byte of 0.83560.</p>

<h2 id="acknowledgements">Acknowledgements</h2>

<p>TPU Development and training supported with Cloud TPUs from Google’s TPU Research Cloud (TRC). Thank you to the excellent TRC team for granting me access to upgraded TPU VMs and for the extensions I received while working on this project!</p>

<div class="footnotes" role="doc-endnotes">
  <ol>
    <li id="fn:1">
      <p>As and aside, its pretty crazy that <a href="https://arxiv.org/abs/2112.11446">Gopher</a> was trained with tensor-parallelism solely using <code class="language-plaintext highlighter-rouge">pmap</code>! <a href="#fnref:1" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
  </ol>
</div>]]></content><author><name>Benjamin Fattori</name></author><category term="jax" /><summary type="html"><![CDATA[TL;DR I improve upon my earlier codebase by implementing ZeRO-1 optimizer sharding using a combination of xmap and pjit. The resultant method is more performant and scales better across multiple TPU hosts achieving 67% MFU on a TPU v3-32. I use this method to train a 1.3B parameter decoder-only transformer on 200B tokens. The codebase I wrote can be found here.]]></summary></entry></feed>